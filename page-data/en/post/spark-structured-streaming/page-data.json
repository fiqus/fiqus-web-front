{
    "componentChunkName": "component---src-blog-templates-blog-post-js",
    "path": "/en/post/spark-structured-streaming",
    "result": {"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"a5b3f836-e74b-509d-9a8b-401dd8c4b46b","html":"<h1>Creating an application with Spark Structured Streaming</h1>\n<h3>Code, configurations and considerations</h3>\n<p><a href=\"https://spark.apache.org/\">Apache Spark</a> is an open-source, distributed processing system used for big data workloads. designed to be fast and resilient.\nIt utilizes in-memory caching and optimized query execution for fast queries against data of any size.\nWith APIs for <code>Java</code>, <code>Scala</code>, <code>Python</code> and <code>R</code>.\nIt has <em>Lazy</em> evaluation, which means any transformation made on the RDDs or Dataframes  creates a logical flow of operations known as <em>Directed Acyclic Graph (DAG)</em> which groups operations to improve efficiency and only will execute the operations when the output is needed.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 684px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/544f7/apache-spark-circulo.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABmUlEQVQoz41SXWvbMBTtL+/b3vcvSvfQp21txqAdhbSlW4MZlJJQiB3ipLFsSZZlfd6rYSv98Eg/zoM4SDq6516dvbALiC94eBV7OzUYgoFHivpD4ggDON88iOIeQ1DO/86uprNpnuf96x12iD1gq21HMFSNqIsFAnrEjBTWGkB8ozKuNuXZ5M55AAAMoSBV04OWZax2MR4nSZKm6f9iABCcMV4rB7Tq4L2PR85Zyli3RSnntZRyIEb0HgRgiN6stUKIENA4b9hGTb7VZG2d344UEQCexd4V4+vPBz+/nyazFakYraSUo5PRyejH0ZfD2/Nj2zDCa75Ip5/209lsvljERra2tdHScNz6dIyx5XJJK0rKShlT17VzrnPYyjjzYc/GWFKKh3WWZayHdS5+jGpbzjml1FqrjFFKvRD3zDBKk0l+8+fq8tL3LZVlKaXU2hBCurIhNE3DGOOcD3oehKxfhTJZvu79odJtFL+VMARAgPhDF3/vvv4ahxA2Yjlf3ZOiigVfTdggoVpp1cZ7AOCce7L6TrY/jn/famVIErhXjAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/e922d/apache-spark-circulo.webp 375w,\n/static/4e23ec14fd006dc3f61b43fa874ea9f0/ffa34/apache-spark-circulo.webp 684w\"\n              sizes=\"(max-width: 684px) 100vw, 684px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/ae393/apache-spark-circulo.png 375w,\n/static/4e23ec14fd006dc3f61b43fa874ea9f0/544f7/apache-spark-circulo.png 684w\"\n            sizes=\"(max-width: 684px) 100vw, 684px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/544f7/apache-spark-circulo.png\"\n            alt=\"Spark ecosystem\"\n            title=\"Spark ecosystem\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Spark ecosystem</figcaption>\n  </figure></p>\n<p>One of the newest features of Spark is the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming</a> that provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.</p>\n<p>You can express your streaming computation the same way you would express a batch computation on static data. The <strong>Spark SQL</strong> engine will take care of running it incrementally, continuously and updating the final result as streaming data continues to arrive. You can use the <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\">Dataset/DataFrame</a> API to express streaming aggregations, event-time windows, stream-to-batch joins.</p>\n<p>For a project we had to make metrics over the last 24 hours period reading incoming log lines, sum the new ones and discarding the old ones and outputting the metrics every ~5 minutes.\nFor solving this problem we used <strong>Structured Streaming</strong> running on an AWS EMR Cluster.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 320px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/8980b/so-apache-spark-in-theory-enough-show-me-the-code.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.875%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAIDBAH/xAAXAQADAQAAAAAAAAAAAAAAAAABAwQC/9oADAMBAAIQAxAAAAGvuaxVM0QZokbjqAP/xAAYEAEBAQEBAAAAAAAAAAAAAAABAgAxEP/aAAgBAQABBQLNePIlZwGe0C//xAAaEQABBQEAAAAAAAAAAAAAAAACAAEQERIx/9oACAEDAQE/AXMb4tjP/8QAFhEBAQEAAAAAAAAAAAAAAAAAEQAQ/9oACAECAQE/ASN//8QAGRAAAwADAAAAAAAAAAAAAAAAARAhACIx/9oACAEBAAY/AnDc2C4H/8QAGxAAAwADAQEAAAAAAAAAAAAAAAERITGBUWH/2gAIAQEAAT8hTa0N9hbkdtuAg+DRZVdYI08y+GguGsw//9oADAMBAAIAAwAAABC4B3//xAAZEQADAAMAAAAAAAAAAAAAAAAAAREhMVH/2gAIAQMBAT8Qc6CltCeCH//EABgRAQADAQAAAAAAAAAAAAAAAAEAEBEx/9oACAECAQE/EBB2brZ//8QAHBABAQEBAAMBAQAAAAAAAAAAAREAITFhkUFx/9oACAEBAAE/EJtIW/3EV5pHvn7m1NvbrsEdL2esBSteqp+OAoldZyIBGboah89Y6VQr4u//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/5530d/so-apache-spark-in-theory-enough-show-me-the-code.webp 320w\"\n              sizes=\"(max-width: 320px) 100vw, 320px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/8980b/so-apache-spark-in-theory-enough-show-me-the-code.jpg 320w\"\n            sizes=\"(max-width: 320px) 100vw, 320px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/8980b/so-apache-spark-in-theory-enough-show-me-the-code.jpg\"\n            alt=\"image\"\n            title=\"image\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">image</figcaption>\n  </figure></p>\n<p>Some simple examples can be found in the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Spark documentation</a>\nBut in real life, solutions will be more complex than that, so here we share a simplified solution of what we have done:</p>\n<deckgo-highlight-code language=\"python\" terminal=\"ubuntu\"  >\n          <code slot=\"code\"># imports\nfrom datetime import datetime\nfrom pyspark.sql.types import StructType, StringType, StructField, \\\n   DoubleType, BooleanType, IntegerType\nfrom pyspark.sql import functions as F\n\n# var definition\nhours_window = 24\nsliding_window = 5\ncheckpoint_dir = &#39;hdfs:///checkpoint_folder/&#39;\n\n# helper to create types\ndef _get_types(types_dict):\n   return StructType([(StructField(f_name, f_type, True)) for f_name, f_type in types_dict.items()])\n\n# structure types for the data streaming\nLOGLINE_SCHEMA = _get_types({\n   &#39;timestamp&#39;: StringType(),\n   &#39;cost&#39;: DoubleType(),\n   &#39;clicks&#39;: DoubleType(),\n})\n\n# read stream definition\nlogline_df = self.spark \\\n   .readStream \\\n   .schema(LOGLINE_SCHEMA) \\\n   .parquet(&#39;hdfs:///mydata/*&#39;)\n\n# convert timestamp field for window operation\nlogline_df = logline_df.withColumn(&#39;timestamp&#39;, F.to_timestamp(&#39;timestamp&#39;))\n\n\n# add watermark definition\nlogline_df = logline_df.withWatermark(&#39;timestamp&#39;, &#39;1 minute&#39;)\n\n\n# compute the metrics\nmetrics_df = logline_df.groupBy(\n   F.window(\n      logline_df.timestamp, f&#39;{hours_window} hours&#39;, \n      f&#39;{sliding_window} minutes&#39;\n   ),\n   logline_df.campaign_eid\n) \\\n.agg(\n   F.sum(&#39;cost&#39;).alias(&#39;cost&#39;),\n   F.sum(&#39;clicks&#39;).alias(&#39;clicks&#39;)\n)\n\n# start streaming\nquery = metrics_df.writeStream \\\n   .option(&quot;checkpointLocation&quot;, checkpoint_dir) \\\n   .foreachBatch(collect_metrics_fn) \\\n   .outputMode(&#39;append&#39;) \\\n   .trigger(processingTime=f&#39;{sliding_window} minutes&#39;) \\\n   .start()\n\n# function called for each batch\ndef collect_metrics_fn(df, epoch_id):\n   # do any operations needed in the dataframe\n   df = df.withColumn(&#39;timestamp&#39;, F.lit(datetime.now()))\n   # output dataframe\n   df.write.mode(&#39;overwrite&#39;).parquet(&#39;hdfs:///output&#39;)</code>\n        </deckgo-highlight-code>\n<p>In the code we can see that we have called <code>.readStream</code>\nto define the source where we are going to read our <a href=\"https://databricks.com/glossary/what-is-parquet\">parquet</a> files, in this case  our path is <code>hdfs:///mydata/</code>.\nIn this example we are using the <em>Hadoop Distributed File System.</em></p>\n<p>More information about HDFS:\n<a href=\"https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\">https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html</a></p>\n<p>And in this section</p>\n<deckgo-highlight-code language=\"python\" terminal=\"ubuntu\"  >\n          <code slot=\"code\">   F.window(\n      logline_df.timestamp, f&#39;{hours_window} hours&#39;, \n      f&#39;{sliding_window} minutes&#39;\n   )</code>\n        </deckgo-highlight-code>\n<p>We define a window period of 24 hours using the <em>timestamp</em> field, and sliding every 5 minutes.</p>\n<p>To start the streaming we need to call the <code>writeStream</code> function.</p>\n<p>The <code>checkpoint</code> folder definition is very important, because if our job fails or gets restarted will try first to recover from the checkpointed data without having to process all the data from scratch again.</p>\n<p>This will not be possible only if the structure of input data or the streaming queries have changed and breaks the compatibility with the checkpoint structure. If this happens the checkpoint folder needs to be removed <em>manually</em> before launching a new job.\nThe <code>outputMode</code> defines how we want the <code>window</code> to be processed: the <code>append</code> mode will output each window only once the period is considered finished (considering the window + the watermark field).\nOn the other hand, the <code>update</code> mode outputs every window each time it has new data that falls into that period.</p>\n<p>More details about <code>output modes</code> can be found <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\">here</a></p>\n<p>And finally the <code>foreachBatch</code> option allows passing a function to be called on each batch. It is possible to do manipulations, like adding more fields and writing the output to a desired location.</p>\n<p>The parameter <code>epoch_id</code> it’s an unique identifier to ensure a only-once guarantee at the moment of writing or processing the data frame.</p>\n<h3>Some useful configurations for long running jobs</h3>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1100px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b9b0687874e23c0e576a988d6dda404a/28a3f/spark-defaults.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAADKklEQVQozwXBWUybBQDA8Y+jrBz9SgujwIBBoVBbKqOsHKXS0FIoIA7G4SwKYjlKAA/CwE03jgTWadgYRQEBG6RAGGMhmct0kTnNEvXBPeiUZIlOiCZmyoPZXnzx7+8nnLMf5kJVApM18UzVp+JrK8TXnM10q5ErrmO8XabG+4KW0+V63ipN50d/C3/deA1/xzECPUU8vd3C4806bng0fDtsQhiriGOiSsWIQ4m3IZON5Y/YWJ5naXKU9aUFrs7PsDzez87NbS4On+Oyd4JPrwXYWl3k5tVP+OerXtjtYX+lmjsDBoTW4yJNOXJOZou8mBNNS34iroKjuHNU7Jwu4fGtPnbHrHz/ei7tuXHkxYjcGrSxHzjFVm8eT+644X4Hu1Nm1trUCE6tSFl6FM4sGc/r5FRrZdjSIpgoioG1Gtjt42DRyb3GRDbqkrDLQwi0GXl6vY5tdxr/fu7i0QclPHi/hEf+agSrOhKHJooanZyXjEpceUrqDQoGjUq+8+j5c9LK7yP57F2w8XDISGuqlPPlav5YrGKrI4sfvIXcfkPLwbVa/rvbgmBLl1GZJeLQyHguNYJclQS7OoJOrZyvOw2wdZK/3zPzZM7Or0PPMp6v4Kw5kZ/HLWx3ZbDmVvPNaBH7c3buj5kQilLCSRFDkUmCORIVxkyHmZ0BM4tVybxTGMdP/QY2alP5rFHDg24t7x6PplMfS322iq7iBAKvanh4ycbenJ0vBvQIUaHBxEpDUYUJvFmSzC/nzcw6klg/kcaQSUW3Ts7egBG8FtZPHMGpkmIQDyERghDDw2kuSKCrUEGvRcm9ETNCtCQYvSKMZGkQq24TXzalcsYgY7P5GRbK40kSBDzZEfQaDuGMEWhPj8QcH0G0NAiHTqShIAGLNg5ThoLK3FiEtMgQMuWhpEiD2Owr5mC4gN9mXmH/Yw8vm3WUWoqpKTFSY7fSVWFltKmc/lMVeOpLGe1poLXSSHWOijJDPFZ9HEKGXEKGGEKmGMIZh5q77kwWBju5PD1Lu9uNzzeN78olVuanuO7zsvLhNMsLM6z6Z/EvLXK220Wj6TANhUepNSbyP5o+yRGWAWZAAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/b9b0687874e23c0e576a988d6dda404a/e922d/spark-defaults.webp 375w,\n/static/b9b0687874e23c0e576a988d6dda404a/3ea8a/spark-defaults.webp 750w,\n/static/b9b0687874e23c0e576a988d6dda404a/cb2d6/spark-defaults.webp 1100w\"\n              sizes=\"(max-width: 1100px) 100vw, 1100px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/b9b0687874e23c0e576a988d6dda404a/ae393/spark-defaults.png 375w,\n/static/b9b0687874e23c0e576a988d6dda404a/e9985/spark-defaults.png 750w,\n/static/b9b0687874e23c0e576a988d6dda404a/28a3f/spark-defaults.png 1100w\"\n            sizes=\"(max-width: 1100px) 100vw, 1100px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/b9b0687874e23c0e576a988d6dda404a/28a3f/spark-defaults.png\"\n            alt=\"image\"\n            title=\"image\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">image</figcaption>\n  </figure></p>\n<p><code>spark.sql.streaming.minBatchesToRetain</code></p>\n<p>This setting indicates the minimum number of checkpoints to be saved for recovery in case of an error.\nThe default it’s <strong>100</strong>, which can make the size of the folder too big. In the case of processing big amounts of data and running for a long period of time, we’ve seen better performance using smaller values, so we had good results with 5.</p>\n<p><code>spark.cleaner.referenceTracking.cleanCheckpoints</code></p>\n<p>This is another important configuration, which by default it’s in false. In true, it will remove the checkpoints that have lost any references.</p>\n<p><code>spark.default.parallelism</code></p>\n<p>Tuning this configuration it’s important, as it defines how much <em>parallelism</em> a job can have, but it depends on the resources available. Too much parallelism will make the task too small and a lot of overhead, and too low parallelism will underutilize the executors.</p>\n<p>A simple math to set a starting value can be the following:\n<code>spark.default.parallelism = spark.executor.instances * spark.executors.cores * 2 (or 3)</code></p>\n<blockquote>\n<p><em>More useful configurations for Structured Streming jobs can be found here:</em></p>\n</blockquote>\n<p><a href=\"https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-properties.html\">https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-properties.html</a></p>\n<h3>How to deploy a yarn application in EMR</h3>\n<p><code>aws emr add-steps --cluster-id ClusterID --steps Type=spark,Name=MyApp,Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.yarn.submit.waitAppCompletion=False,--conf,spark.yarn.appMasterEnv.profile=$(profile),--py-files,s3://my-bucket/app-dependencies.zip,s3://my-bucket/my_app.py],ActionOnFailure=CONTINUE</code></p>\n<p>With <code>aws emr add-steps</code> command we can add a step that will create an application, and setting <code>waitAppCompletion=False</code> the step will not wait the app to finish. Being a streaming job, ideally will not finish.</p>\n<p>And with this parameter:\n<code>spark.yarn.appMasterEnv.profile</code></p>\n<p>We can send environment variables to the application, like for example the profile (<em>staging / produccion</em>), here the example on how to get it in the code:</p>\n<p><code>PROFILE = os.getenv('profile', 'staging')</code></p>\n<p><strong>Spark Structured streaming</strong> is a great tool, but requires some effort and knowledge to get good results. We hope these insights will help you in this venture.</p>","frontmatter":{"lang":"en","type":"post","title":"Spark Structured Streaming","author":"Agustin Recouso","slug":"/spark-structured-streaming","date":"2022-01-24","tags":["python","spark","big-data"],"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACe0lEQVQ4y5VT30/TUBjtH2diIo8YSiIPQmQQS3RB0ABLEPoATH4NHWNbqaLDkcgDEXBCRmFhhsGWjHUgI9nKWEkV19lu7ehc23sN6VzMshg5ubn57s05+b6bew4CbwgAANC16wUAciPRj9BmWRKNI4QQqcOpAYRA1yCEv35e7nXeOlt6VRb47MGWcslWxLqu/2HWbwshZNc9zOLUmXfmxPY0hN0uXqRqOxcK+VwuJ4qiLMv5vCgKQkEp8XTooKcx4xkvC1kIYfLNWC4arIwNACBJsru7m2VZmqYTiZNwOBwMBjmOo2OH++HI1bfM97ej2+7xXF4CmpqwDwhHBxUxRVEoim5sbLBsxufzzc05eJ5X1bLX+54kyWKxmEyff6a2n/U+cRDEqtsW72s6sVsqYkmSULTJ7/cvL38YGhqamprE8WGHY3ZiYhzH8ZHRkd7eHrfL2Xi30WQylRTliktfXV5ci1VV5TguFAo1NNxpb3+QyWQYhsGwh62t92VZZhimpeWexTKQTCabm5vMZvN5Om28tCLGMKytrdVmm0ZRtKPDZDY/Pj1NeDzvurowq3UMx4fn54loNGq1jlHUVn9/nyiKpVKpMrYgCCzLplLJwcFBnud3AgFqJ5BimHWfb+2TL0bHw5FIYHdXkiQIIc/zsiyDqkmMSlGUbPb6J/bDkVkn4SLIF5PTxOuFWafb7nQteBarzCqQqquqDtN1XdM0Y19ZXdv0U0fHXw9jdIyOf9kLxY+OJUk2mEiNN2s8s/JxzeNdemmfszwfnrE7HC6is+tRjI4bpkTqRqBaa5qmqioAQNM0457n+UJBqtP5H4mqe4/8Vxb/ghEhQ/wb4/dw1+9YmkwAAAAASUVORK5CYII=","aspectRatio":1.1627906976744187,"src":"/static/e417585de76300a95cd1db51557236ba/1c1a5/sparkstrucutredstreaming.png","srcSet":"/static/e417585de76300a95cd1db51557236ba/69585/sparkstrucutredstreaming.png 200w,\n/static/e417585de76300a95cd1db51557236ba/497c6/sparkstrucutredstreaming.png 400w,\n/static/e417585de76300a95cd1db51557236ba/1c1a5/sparkstrucutredstreaming.png 550w","sizes":"(max-width: 550px) 100vw, 550px"}}},"imageCredits":"unsplash"},"timeToRead":6}},{"node":{"id":"d1985f9d-ed7f-5bbe-ac6d-11ec68f914eb","html":"<h1>Creando una aplicacion con Spark Structured Streaming</h1>\n<h3>Codigo, configuraciones y consejos</h3>\n<p><a href=\"https://spark.apache.org/\">Apache Spark</a> es un framework de programación open-source para procesar datos masivos o big data, de forma distribuida, diseñado para ser rápido, y tolerante a fallas.\nTrabaja en memoria, con lo que se consigue mucha mayor velocidad de procesamiento.\nProporciona APIs para los lenguajes <code>Java</code>, <code>Scala</code>, <code>Python</code> y <code>R</code>.</p>\n<p>Utiliza la evaluación perezosa (<em>lazy</em>), lo que significa es que todas las transformaciones que vamos realizando sobre los RDD o Dataframes, no se resuelven, si no que se van almacenando en un grafo acíclico dirigido (<em>llamado DAG</em>), y cuando ejecutamos una acción, es decir, cuando la herramienta no tenga más opción que realizar todas las transformaciones, será cuando se ejecuten</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 684px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/544f7/apache-spark-circulo.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.4%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABmUlEQVQoz41SXWvbMBTtL+/b3vcvSvfQp21txqAdhbSlW4MZlJJQiB3ipLFsSZZlfd6rYSv98Eg/zoM4SDq6516dvbALiC94eBV7OzUYgoFHivpD4ggDON88iOIeQ1DO/86uprNpnuf96x12iD1gq21HMFSNqIsFAnrEjBTWGkB8ozKuNuXZ5M55AAAMoSBV04OWZax2MR4nSZKm6f9iABCcMV4rB7Tq4L2PR85Zyli3RSnntZRyIEb0HgRgiN6stUKIENA4b9hGTb7VZG2d344UEQCexd4V4+vPBz+/nyazFakYraSUo5PRyejH0ZfD2/Nj2zDCa75Ip5/209lsvljERra2tdHScNz6dIyx5XJJK0rKShlT17VzrnPYyjjzYc/GWFKKh3WWZayHdS5+jGpbzjml1FqrjFFKvRD3zDBKk0l+8+fq8tL3LZVlKaXU2hBCurIhNE3DGOOcD3oehKxfhTJZvu79odJtFL+VMARAgPhDF3/vvv4ahxA2Yjlf3ZOiigVfTdggoVpp1cZ7AOCce7L6TrY/jn/famVIErhXjAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/e922d/apache-spark-circulo.webp 375w,\n/static/4e23ec14fd006dc3f61b43fa874ea9f0/ffa34/apache-spark-circulo.webp 684w\"\n              sizes=\"(max-width: 684px) 100vw, 684px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/ae393/apache-spark-circulo.png 375w,\n/static/4e23ec14fd006dc3f61b43fa874ea9f0/544f7/apache-spark-circulo.png 684w\"\n            sizes=\"(max-width: 684px) 100vw, 684px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/4e23ec14fd006dc3f61b43fa874ea9f0/544f7/apache-spark-circulo.png\"\n            alt=\"image\"\n            title=\"image\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">image</figcaption>\n  </figure></p>\n<p>Una de las más novedosas e interesantes funcionalidades de Spark es el <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Structured Streaming</a>, que permite procesar datos de forma escalable, tolerante a fallos y continua.</p>\n<p>La idea de Structure Streaming es procesar la data en tiempo real como si fuera una tabla que continuamente se actualiza con los nuevos valores, y permite realizar agregaciones sobre la misma.\nLos datos deben mantener una estructura definida, y el beneficio es que se pueden realizar operaciones basadas en el tiempo para decidir si los valores se deben considerar o descartar.</p>\n<p>Como ejemplo, para un proyecto debimos realizar unos cálculos en una ventana de las últimas 24 horas (sliding window), agregando los valores nuevos recibidos y descartando los viejos para obtener unas métricas que se actualizan cada 5 minutos.\nPara esto utilizamos la funcionalidad de Structured Streaming de Spark corriendo en un EMR cluster en AWS.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 320px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/8980b/so-apache-spark-in-theory-enough-show-me-the-code.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.875%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAIDBAH/xAAXAQADAQAAAAAAAAAAAAAAAAABAwQC/9oADAMBAAIQAxAAAAGvuaxVM0QZokbjqAP/xAAYEAEBAQEBAAAAAAAAAAAAAAABAgAxEP/aAAgBAQABBQLNePIlZwGe0C//xAAaEQABBQEAAAAAAAAAAAAAAAACAAEQERIx/9oACAEDAQE/AXMb4tjP/8QAFhEBAQEAAAAAAAAAAAAAAAAAEQAQ/9oACAECAQE/ASN//8QAGRAAAwADAAAAAAAAAAAAAAAAARAhACIx/9oACAEBAAY/AnDc2C4H/8QAGxAAAwADAQEAAAAAAAAAAAAAAAERITGBUWH/2gAIAQEAAT8hTa0N9hbkdtuAg+DRZVdYI08y+GguGsw//9oADAMBAAIAAwAAABC4B3//xAAZEQADAAMAAAAAAAAAAAAAAAAAAREhMVH/2gAIAQMBAT8Qc6CltCeCH//EABgRAQADAQAAAAAAAAAAAAAAAAEAEBEx/9oACAECAQE/EBB2brZ//8QAHBABAQEBAAMBAQAAAAAAAAAAAREAITFhkUFx/9oACAEBAAE/EJtIW/3EV5pHvn7m1NvbrsEdL2esBSteqp+OAoldZyIBGboah89Y6VQr4u//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/5530d/so-apache-spark-in-theory-enough-show-me-the-code.webp 320w\"\n              sizes=\"(max-width: 320px) 100vw, 320px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/8980b/so-apache-spark-in-theory-enough-show-me-the-code.jpg 320w\"\n            sizes=\"(max-width: 320px) 100vw, 320px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/8f39fa53a1fc0b66872fbf6dadd8cd48/8980b/so-apache-spark-in-theory-enough-show-me-the-code.jpg\"\n            alt=\"image\"\n            title=\"image\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">image</figcaption>\n  </figure></p>\n<p>Algunos ejemplos iniciales se puede encontrar en la <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">documentación de spark</a></p>\n<p>Pero en la vida real suelen ser un poco más complejos, aquí les compartimos una versión simplificada de lo que hicimos:</p>\n<deckgo-highlight-code language=\"python\" terminal=\"ubuntu\"  >\n          <code slot=\"code\"># imports\nfrom datetime import datetime\nfrom pyspark.sql.types import StructType, StringType, StructField, \\\n   DoubleType, BooleanType, IntegerType\nfrom pyspark.sql import functions as F\n\n# definicion de variables\nhours_window = 24\nsliding_window = 5\ncheckpoint_dir = &#39;hdfs:///checkpoint_folder/&#39;\n\n# funcion para crear tipos facilmente\ndef _get_types(types_dict):\n   return StructType([(StructField(f_name, f_type, True)) for f_name, f_type in types_dict.items()])\n\n# definimos los tipos para la data que recibiremos\nLOGLINE_SCHEMA = _get_types({\n   &#39;timestamp&#39;: StringType(),\n   &#39;cost&#39;: DoubleType(),\n   &#39;clicks&#39;: DoubleType(),\n})\n\n# definimos del lector de streaming\nlogline_df = self.spark \\\n   .readStream \\\n   .schema(LOGLINE_SCHEMA) \\\n   .parquet(&#39;hdfs:///mydata/*&#39;)\n\n# convertimos en timestamp para poder usar la funcion window\nlogline_df = logline_df.withColumn(&#39;timestamp&#39;, F.to_timestamp(&#39;timestamp&#39;))\n\n\n# agregamos definicion de watermark\nlogline_df = logline_df.withWatermark(&#39;timestamp&#39;, &#39;1 minute&#39;)\n\n\n# calculamos las metricas que nos interesan \nmetrics_df = logline_df.groupBy(\n   F.window(\n      logline_df.timestamp, f&#39;{hours_window} hours&#39;, \n      f&#39;{sliding_window} minutes&#39;\n   ),\n   logline_df.campaign_eid\n) \\\n.agg(\n   F.sum(&#39;cost&#39;).alias(&#39;cost&#39;),\n   F.sum(&#39;clicks&#39;).alias(&#39;clicks&#39;)\n)\n\n\n# iniciamos el proceso de streaming\nquery = metrics_df.writeStream \\\n   .option(&quot;checkpointLocation&quot;, checkpoint_dir) \\\n   .foreachBatch(collect_metrics_fn) \\\n   .outputMode(&#39;append&#39;) \\\n   .trigger(processingTime=f&#39;{sliding_window} minutes&#39;) \\\n   .start()\n\n# funcion llamada para cada iteracion (batch)\n# aqui podemos manipular el df final que tendremos\ndef collect_metrics_fn(df, epoch_id):\n   # agregamos una columna\n   df = df.withColumn(&#39;timestamp&#39;, F.lit(datetime.now()))\n   # escribimos el dataframe en HDFS en formato parquet\n   df.write.mode(&#39;overwrite&#39;).parquet(&#39;hdfs:///output&#39;)</code>\n        </deckgo-highlight-code>\n<p>En el código podemos ver que en la función <code>.readStream</code>\ndefinimos que vamos a leer archivos del tipo parquet desde el path local en <code>/mydata/</code>\nEn este ejemplo estamos leyendo desde los archivos distribuidos Hadoop (HDFS).</p>\n<p>Más informacion acerca de HDFS:\n<a href=\"https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\">https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html</a></p>\n<p>Con la definición de:</p>\n<deckgo-highlight-code language=\"python\" terminal=\"ubuntu\"  >\n          <code slot=\"code\">   F.window(\n      logline_df.timestamp, f&#39;{hours_window} hours&#39;, \n      f&#39;{sliding_window} minutes&#39;\n   )</code>\n        </deckgo-highlight-code>\n<p>creamos una ventana de tiempo sobre el campo timestamp de 24 horas (hours_window), que se va a desplazar cada 5 minutos.</p>\n<p>Para comenzar con el procesamiento debemos llamar a la función <code>writeStream</code>.\nLa definición de la carpeta <code>checkpoint</code> es importante, ya que si nuestro trabajo de streaming falla o es deployado nuevamente, intentará primero iniciar el trabajo desde el último checkpoint guardado, sin tener que procesar todos los datos nuevamente; siempre y cuando no haya cambios la estructura de la query que rompan la compatibilidad con el checkpoint. De ser así, es necesario remover la carpeta <em>manualmente</em> antes de iniciar nuevamente el proceso.</p>\n<p>El <code>outputMode</code>  define como queremos que las ventanas sean procesadas, con el modo <code>append</code> el resultado de cada ventana es escrito una sola vez, al finalizar el periodo definido en el <code>Watermark</code>, de esta forma cada ventana escrita es siempre final ya que no se espera que más datos puedan ingresar en esa ventana.</p>\n<p>A diferencia en el modo <code>update</code> si hay nuevos datos que entran en una ventana se re-escribe en el output la misma ventana.\nPueden ver más información en la <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\">documentación de spark</a> sobre los <code>output mode</code> de las ventanas disponibles.</p>\n<p>Y finalmente en la sección <code>foreachBatch</code> se puede agregar una función para ser llamada en cada iteración donde se puede manipular, agregar más información y escribir el dataframe al destino y en el formato necesario. El parametro <code>epoch_id</code> sirve para identificar unívocamente cada iteración y tener una garantia de escribir los datos una sola vez si es necesario.</p>\n<h3>Algunas configuraciones útiles para trabajos que corran por mucho tiempo</h3>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1100px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b9b0687874e23c0e576a988d6dda404a/28a3f/spark-defaults.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAADKklEQVQozwXBWUybBQDA8Y+jrBz9SgujwIBBoVBbKqOsHKXS0FIoIA7G4SwKYjlKAA/CwE03jgTWadgYRQEBG6RAGGMhmct0kTnNEvXBPeiUZIlOiCZmyoPZXnzx7+8nnLMf5kJVApM18UzVp+JrK8TXnM10q5ErrmO8XabG+4KW0+V63ipN50d/C3/deA1/xzECPUU8vd3C4806bng0fDtsQhiriGOiSsWIQ4m3IZON5Y/YWJ5naXKU9aUFrs7PsDzez87NbS4On+Oyd4JPrwXYWl3k5tVP+OerXtjtYX+lmjsDBoTW4yJNOXJOZou8mBNNS34iroKjuHNU7Jwu4fGtPnbHrHz/ei7tuXHkxYjcGrSxHzjFVm8eT+644X4Hu1Nm1trUCE6tSFl6FM4sGc/r5FRrZdjSIpgoioG1Gtjt42DRyb3GRDbqkrDLQwi0GXl6vY5tdxr/fu7i0QclPHi/hEf+agSrOhKHJooanZyXjEpceUrqDQoGjUq+8+j5c9LK7yP57F2w8XDISGuqlPPlav5YrGKrI4sfvIXcfkPLwbVa/rvbgmBLl1GZJeLQyHguNYJclQS7OoJOrZyvOw2wdZK/3zPzZM7Or0PPMp6v4Kw5kZ/HLWx3ZbDmVvPNaBH7c3buj5kQilLCSRFDkUmCORIVxkyHmZ0BM4tVybxTGMdP/QY2alP5rFHDg24t7x6PplMfS322iq7iBAKvanh4ycbenJ0vBvQIUaHBxEpDUYUJvFmSzC/nzcw6klg/kcaQSUW3Ts7egBG8FtZPHMGpkmIQDyERghDDw2kuSKCrUEGvRcm9ETNCtCQYvSKMZGkQq24TXzalcsYgY7P5GRbK40kSBDzZEfQaDuGMEWhPj8QcH0G0NAiHTqShIAGLNg5ThoLK3FiEtMgQMuWhpEiD2Owr5mC4gN9mXmH/Yw8vm3WUWoqpKTFSY7fSVWFltKmc/lMVeOpLGe1poLXSSHWOijJDPFZ9HEKGXEKGGEKmGMIZh5q77kwWBju5PD1Lu9uNzzeN78olVuanuO7zsvLhNMsLM6z6Z/EvLXK220Wj6TANhUepNSbyP5o+yRGWAWZAAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/b9b0687874e23c0e576a988d6dda404a/e922d/spark-defaults.webp 375w,\n/static/b9b0687874e23c0e576a988d6dda404a/3ea8a/spark-defaults.webp 750w,\n/static/b9b0687874e23c0e576a988d6dda404a/cb2d6/spark-defaults.webp 1100w\"\n              sizes=\"(max-width: 1100px) 100vw, 1100px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/b9b0687874e23c0e576a988d6dda404a/ae393/spark-defaults.png 375w,\n/static/b9b0687874e23c0e576a988d6dda404a/e9985/spark-defaults.png 750w,\n/static/b9b0687874e23c0e576a988d6dda404a/28a3f/spark-defaults.png 1100w\"\n            sizes=\"(max-width: 1100px) 100vw, 1100px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/b9b0687874e23c0e576a988d6dda404a/28a3f/spark-defaults.png\"\n            alt=\"image\"\n            title=\"image\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">image</figcaption>\n  </figure></p>\n<p><code>spark.sql.streaming.minBatchesToRetain</code></p>\n<p>Esta configuración indica la cantidad mínima de checkpoints que se van a guardar para hacer un recovery en caso de que haya un error.\nEl default es 100, pero puede hacer crecer mucho la carpeta del checkpoint en caso de que sean muchos datos, por esto nosotros utilizamos un valor de 5.</p>\n<p><code>spark.cleaner.referenceTracking.cleanCheckpoints</code></p>\n<p>También esta configuración que por default está en false, cambiando a true elimina los checkpoints que se encuentran sin referencia.</p>\n<p><code>spark.default.parallelism</code></p>\n<p>Esto indica cuanto paralelismo puede tener un trabajo, lo cual depende del hardware provisionado. Mucho paralelismo hará que las tareas sean muy chicas y genere mucho overhead y poco hará que se sub-utilice los recursos de los executors.</p>\n<p>Un cálculo simple para saber qué valor utilizar sería el siguiente:\n<code>spark.default.parallelism = spark.executor.instances * spark.executors.cores * 2 (or 3)</code></p>\n<p><em>Más configuraciones que podrían ser útiles para trabajos de Structured Streaming pueden ser encontradas aqui</em>\n<a href=\"https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-properties.html\">https://jaceklaskowski.gitbooks.io/spark-structured-streaming/content/spark-sql-streaming-properties.html</a></p>\n<h3>Deployar una aplicación Spark en EMR con yarn</h3>\n<p><code>aws emr add-steps --cluster-id ClusterID --steps Type=spark,Name=MyApp,Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.yarn.submit.waitAppCompletion=False,--conf,spark.yarn.appMasterEnv.profile=$(profile),--py-files,s3://my-bucket/app-dependencies.zip,s3://my-bucket/my_app.py],ActionOnFailure=CONTINUE</code></p>\n<p>Con el comando aws emr add-steps podemos agregar un step que va a crear una aplicación, y al elegir <code>waitAppCompletion=False</code> el step terminara al deployar, al ser un job de streaming idealmente no tendra una finalización.</p>\n<p>Y con este parámetro:\n<code>spark.yarn.appMasterEnv.profile</code></p>\n<p>Podemos pasar variables de entorno a la aplicación para su ejecución, como por ejemplo el profile (<em>staging / producción</em>), un ejemplo de como usar esa variable del código en python:</p>\n<p><code>PROFILE = os.getenv('profile', 'staging')</code></p>\n<p><strong>Spark structured streaming</strong> es una gran herramienta pero requiere esfuerzo y conocimiento para poder obtener resultados satisfactorios. Esperamos que les sirva a los nuevos aventurados en el tema.</p>","frontmatter":{"lang":"es","type":"post","title":"Spark Strucutred Streaming","author":"Agustin Recouso","slug":"/spark-structured-streaming","date":"2022-01-24","tags":["python","spark","big-data"],"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAACe0lEQVQ4y5VT30/TUBjtH2diIo8YSiIPQmQQS3RB0ABLEPoATH4NHWNbqaLDkcgDEXBCRmFhhsGWjHUgI9nKWEkV19lu7ehc23sN6VzMshg5ubn57s05+b6bew4CbwgAANC16wUAciPRj9BmWRKNI4QQqcOpAYRA1yCEv35e7nXeOlt6VRb47MGWcslWxLqu/2HWbwshZNc9zOLUmXfmxPY0hN0uXqRqOxcK+VwuJ4qiLMv5vCgKQkEp8XTooKcx4xkvC1kIYfLNWC4arIwNACBJsru7m2VZmqYTiZNwOBwMBjmOo2OH++HI1bfM97ej2+7xXF4CmpqwDwhHBxUxRVEoim5sbLBsxufzzc05eJ5X1bLX+54kyWKxmEyff6a2n/U+cRDEqtsW72s6sVsqYkmSULTJ7/cvL38YGhqamprE8WGHY3ZiYhzH8ZHRkd7eHrfL2Xi30WQylRTliktfXV5ci1VV5TguFAo1NNxpb3+QyWQYhsGwh62t92VZZhimpeWexTKQTCabm5vMZvN5Om28tCLGMKytrdVmm0ZRtKPDZDY/Pj1NeDzvurowq3UMx4fn54loNGq1jlHUVn9/nyiKpVKpMrYgCCzLplLJwcFBnud3AgFqJ5BimHWfb+2TL0bHw5FIYHdXkiQIIc/zsiyDqkmMSlGUbPb6J/bDkVkn4SLIF5PTxOuFWafb7nQteBarzCqQqquqDtN1XdM0Y19ZXdv0U0fHXw9jdIyOf9kLxY+OJUk2mEiNN2s8s/JxzeNdemmfszwfnrE7HC6is+tRjI4bpkTqRqBaa5qmqioAQNM0457n+UJBqtP5H4mqe4/8Vxb/ghEhQ/wb4/dw1+9YmkwAAAAASUVORK5CYII=","aspectRatio":1.1627906976744187,"src":"/static/e417585de76300a95cd1db51557236ba/1c1a5/sparkstrucutredstreaming.png","srcSet":"/static/e417585de76300a95cd1db51557236ba/69585/sparkstrucutredstreaming.png 200w,\n/static/e417585de76300a95cd1db51557236ba/497c6/sparkstrucutredstreaming.png 400w,\n/static/e417585de76300a95cd1db51557236ba/1c1a5/sparkstrucutredstreaming.png 550w","sizes":"(max-width: 550px) 100vw, 550px"}}},"imageCredits":"unsplash"},"timeToRead":6}}]}},"pageContext":{"slug":"/spark-structured-streaming","language":"en","intl":{"language":"en","languages":["es","en"],"messages":{"Home":"Home","Servicios":"Services","Cultura":"Culture","Labs":"Labs","Blog":"Blog","Articles":"Articles","footer.designby":"Intercooperative work made by","footer.coopName":"el maizal and nayra","footer.coopName1":"el maizal","footer.coopName2":"nayra","footer.coopName1Path":"https://elmaizal.coop.ar","footer.coopName2Path":"https://nayra.coop/en","footer.preposition":"and","footer.disclaimer":"Unless stated otherwise, content on this site is licensed under ","footer.licenseName":"Creative Commons Attribution","footer.licenseIconAlt":"Creative Commons Attribution logo","header.logoAlt":"logo Fiqus","header.iconTogglerAlt":"icon to open menu","header.iconTogglerCloseAlt":"icon to close menu","homepage.title":"Homepage","homepage.imageAlt":" ","homepage.imageAltMobile":" ","homepage.leadLine1":"We are a ","homepage.leadUnderlined":"software cooperative ","homepage.leadLine2":"We are ","homepage.leadBold":"collective construction","homepage.leadLine3":", culture and minds in constant motion.","homepageCulture.title":"Culture","homepageCulture.birdImage":"bird.svg","homepageCulture.birdImageAlt":"illustration of a bird","homepageCulture.image":"homepageCulture.svg","homepageCulture.imageAlt":"illustration depicting the culture section","homepageCulture.descriptionLine1":"We have a superpower: We own the company we work for! Fiqus is not just another job, it's a ","homepageCulture.underlinedText":"life project","homepageCulture.descriptionLine2":" In which we have built a consolidated team of professionals who are in continuous training to be able to provide high quality technical solutions.","homepageCulture.btnText":"view more","homepageLabs.title":"Labs","homepageLabs.subtitle":"We train permanently!","homepageLabs.image":"homepageLabs.svg","homepageLabs.imageAlt":"illustration representing the culture section","homepageLabs.imageAltMobile":"illustration representing the culture section","homepageLabs.descriptionLine1":"One of the objectives of Fiqus is to constantly train ourselves to specialize in cutting-edge technologies, that's why every week we have","homepageLabs.descriptionBold1":" FiqusLabs","homepageLabs.descriptionLine2":"  a hands-on learning space where we investigate cutting-edge technologies such as","homepageLabs.descriptionBold2":" Elixir","homepageLabs.descriptionComma":",","homepageLabs.descriptionBold3":" Erlang","homepageLabs.descriptionLine4":" and","homepageLabs.descriptionBold4":" MachineLearning","homepageLabs.descriptionDot":".","homepageLabs.btnText":"view more","button.verMas":"view more","button.send":"send","button.read":"read","verMasArticulos":"view all articles","y":" and ","contactForm.title":"Contact us","contactForm.nameField":"Name","contactForm.emailField":"E-mail","contactForm.requiredFieldError":"Please complete the required field","contactForm.textAreaField":"Message","contactForm.messageSent":"Your message has been sent.","contactForm.thankYou":"Thank you for contacting us","contactForm.email":"info@fiqus.coop","contactForm.sedes":"Locations","contactForm.office1":"14 de Julio 1268 Ciudad de Buenos Aires, Argentina","contactForm.office2":"Las Frutillas 109, Casa 2 Villa La Angostura, Neuquén, Argentina","services.title":"Services","services.tagsTitle":"Technologies","services.btnText":"go to services","service_datos.service":"Data Science","service_datos.description":"This area is dedicated to the cleaning, extraction and analysis of data in order to feed Artificial Intelligence or Machine Learning processes. It is essential to process the data in order to make it easy to understand, as well as its use to reach the proposed goals.","service_datos.tags.0":"Python","service_datos.tags.1":"Pandas","service_datos.tags.2":"Numpy","service_datos.tags.3":"Seaborn","service_datos.image":"data","service_datos.link":" ","service_inteligenciaArtificial.service":"Artificial Intelligence","service_inteligenciaArtificial.description":"Machine Learning is a subfield of Artificial Intelligence in which the goal is for computers to learn. From collected and processed data, we design, build and train models that allow us to predict future events, classify images, recognize entities in text and much more.","service_inteligenciaArtificial.tags.0":"Python","service_inteligenciaArtificial.tags.1":"Pandas","service_inteligenciaArtificial.tags.2":"Numpy","service_inteligenciaArtificial.tags.3":"Seaborn","service_inteligenciaArtificial.image":"inteligenciaArtificial","service_inteligenciaArtificial.link":" ","service_altaConcurrencia.service":"High Concurrence","service_altaConcurrencia.description":"We use functional programming languages in real time and widely adopted in the market, which support very high levels of concurrency and distribution, producing systems that provide high speed response to high demands.","service_altaConcurrencia.tags.0":"Earlang","service_altaConcurrencia.tags.1":"Elixir","service_altaConcurrencia.tags.2":"Phoenix Framework","service_altaConcurrencia.tags.3":"LiveView","service_altaConcurrencia.image":"altaConcurrencia","service_altaConcurrencia.link":" ","service_blockchain.service":"Blockchain | Fintech","service_blockchain.description":"We are developing fintech, which are new applications, processes, products or business models in the financial services industry composed of one or more complementary financial services that are offered to the public through the Internet.","service_blockchain.tags.0":"Python","service_blockchain.tags.1":"Ethereum","service_blockchain.image":"blockchain","service_blockchain.link":" ","service_fullstack.service":"Fullstack","service_fullstack.description":"We develop web/mobile/desktop applications with different technologies to solve problems of organizations, cooperatives and companies in order to improve the efficiency of their processes.","service_fullstack.tags.0":"Django","service_fullstack.tags.1":"Phoenix","service_fullstack.tags.2":"Phoenix LiveView","service_fullstack.tags.3":"ReactJS","service_fullstack.tags.4":"VueJS","service_fullstack.tags.5":"React Native","service_fullstack.image":"fullstack","service_fullstack.link":" ","service_subsection.title":"Training","service_subsection.description":"We provide technical and/or cooperative training for organizations, cooperatives and companies in which we seek to share in a friendly and systematized way the knowledge obtained in the technological and cooperative fields.","service_subsection.image":"capacitacion","service_subsection.featuredServiceImageAlt":"Training animated image","service_subsection.link":"servicios/#capacitacion","culture.title":"Culture","culture.subtitle":"We have a superpower: We own the company we work for!","culture.description":"Fiqus is not just another job, it is a life project, in which we have built a consolidated team of professionals who are in continuous training to provide high quality technical solutions.","culture_historia.title":"History","culture_historia.imageMobile":"equipoMobile","culture_historia.imageAltMobile":"equipo Fiqus","culture_historia.imageDesktop":"equipoDesktop","culture_historia.imageAlt":"Fiqus team","culture_historia.content_line1":"Fiqus is like a big family, which started when several engineering students from the National Technological University got tired of traditional jobs and began to think of their own construction, based on different production approaches to those of the companies they had previously worked for","culture_historia.content_line2":"After several months of preparation, the idea of building something self-managed, horizontal, democratic and collectively owned became stronger and stronger. That is how the idea of building a social enterprise was born, that is how Fiqus was born.","culture_historia.content_line3":"Over the years, we have built an organizational structure that adapts to people and not the other way around. This is how, as we went along and based on the experience acquired, we built a series of agreements that today are part of our internal regulations. Our rules and regulations are not static, they are living things that evolve according to the evolution of the people who are part of our organization. This is where we reflect our concrete proposals to ensure that the people who participate in the collective are placed at the center of the production processes.","culture_historia.content_line4":"We like to think that Fiqus is a proposal that invites those who are not familiar with technological cooperativism to see that a paradigm shift is possible.","culture_historia.subtitle":"We are organized horizontally and democratically!","culture_historia.content_line5":"The decisions of the cooperative are made at the Ordinary General Assembly once a year. In addition, as a collective organization, twice a year we hold a meeting where we think, based on our personal desires, about our objectives as a cooperative and we meet weekly (virtually) to plan the daily tasks. ","culture_federales.title1":"We operate at federal level!","culture_federales.image":"mapa_argentina","culture_federales.imageAlt":" ","culture_federales.content1":"Our current distribution is the result of having created a structure malleable enough to be able to adapt to people's needs. In this way, as we have grown as people and as we have gone through the different scenarios that life has presented us with, we have prioritized accompanying the collective by providing the necessary support so that everyone can decide, for example, where and how to live.","culture_federales.tenemosSedes":"We currently have offices in ","culture_federales.y":"and","culture_federales.personasAsociadas":"But in addition, we have associates working from ","culture_federales.title2":"Territorial","culture_federales.content2":"As a worker cooperative, we understand ourselves as territorial social actors. Based on the seventh cooperative principle of 'Commitment to the Community', we assume the responsibility of extending the collective construction that we carry out day by day within the cooperative in the territory belonging to one of our locations. That is why we work actively in the Associativism Board of Villa La Angostura, Neuquén. There we meet periodically together with other actors of the social economy as well as with individuals committed to the local community to develop projects that enhance local initiatives, always with a collaborative and horizontal approach.","culture_facttic.title":"We are FACTTIC members!","culture_facttic.subtitle":"The federation is a national organization made up of technology cooperatives from all over Argentina.","culture_facttic.content_line1_part1":"It all started when the first cooperatives in the sector began to get in touch with each other because we understood that there is strength in unity to build a model that can scale with the goal of gradually becoming a real alternative to the traditional production model. As a result of these first interactions, the idea of signing a letter of commitment was born to found the ","culture_facttic.content_line1_facttic":"Argentine Federation of Technology, Innovation and Knowledge Cooperatives (FACTTIC)","culture_facttic.content_line1_part2":", a space for collective construction that fosters solidarity and collaboration as the driving forces behind the creation of technology with cooperative values.","culture_facttic.content_line2":"The continuous interaction with other cooperatives in our sector and the joint work to develop common strategies and share knowledge generates a continuously evolving virtuous circle that drives us to grow and continue to strengthen our work.","culture_facttic.content_line3":"The federation allows us to be in contact with other realities, allows us to break with the endogamy of our organization and invites us to think of ourselves as something bigger, covering a larger scale. It is a space of collective creation where imagination applied to collaborative construction finds no limits.","culture_internacional.title":"International","culture_internacional.content_line1":"Several years after participating in the creation of the federation, we began to contact technology cooperatives in other parts of the world with the objective of sharing the collaboration and intercooperation model built with local cooperatives on a more global scale.","culture_internacional.content_line2":"After several interactions, we made our network's first international inter-cooperative trip. We traveled to the United Kingdom to meet technology cooperatives from a local network. We shared experiences and began to build the foundations of what is now the Global Community of Technology Worker Cooperatives.","culture_internacional.content_line3":"The network brings together more than 50 technology cooperatives from around the world, with presence in 16 countries from three different continents. We share weekly meetings where we learn about our cultures, discuss our common goals and outline a roadmap to grow the community that composes it. Little by little, we begin to intercooperate with the objective of strengthening our local constructions with a global scale. In this way, organized, technology cooperatives will be able to help each other through cooperation to sell and develop projects together.","labs.title":"Labs","labs.subtitle":"FiqusLabs is a space where we research new technologies and train ourselves.","labs.content":"In addition, learning a new technology often comes with the ability to implement a project that benefits the community or the cooperative environment.","casos_de_exito.title":"Case Studies","casos_de_exito.tagsTitle":"Researched Technologies","casos_de_exito.imageAltLine1":"project image","casos_de_exito.btnTextVerMas":"view more","casos_de_exito.btnTextGithub":"go to Github","blog.title":"Blog","blogPost.tagsTitle":"Related tags","blogPost.relatesPost":"Related articles","blogPost.verTodosBtn":"See all articles","page404.title":"Error 404","page404.descriptionLine1":"We couldn't find what you were looking for. If you need help, please contact us.","page404.descriptionLine2":"We invite you to continue browsing our site by clicking on the following button","page404.btnText":"go back to home","page404.btnHref":"/","page404.imageAlt":"illustration fiqus tree","page404.imageAltMobile":"illustration fiqus tree"},"routed":true,"originalPath":"/post/spark-structured-streaming","redirect":true,"redirectDefaultLanguageToRoot":false,"defaultLanguage":"es","fallbackLanguage":"es","ignoredPaths":[]}}},
    "staticQueryHashes": ["632761789"]}